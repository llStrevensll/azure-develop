azure cli
https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest

az login
az storage account list

---------
Powershell
Set-ExecutionPolicy RemoteSigned 
Install-Module -Name Az -AllowClobber -Scope CurrentUser
Import-Module Az -Verbose 

Connect-AzAccount

------
Create vm
Subscription
   - Resource group

instance details
 Virtual machine name
 Region
 Availability options
 Image
 Azure Spot instance
 Size

administrator account
 username
 password
 confirm password

inbound port rules
 public inbound ports
 select inbound ports RDP(3389)

Disk,
Networking,

So a subnet is again a logical separation of the address space of the virtual Network.
Then we have a public IP address, so the public IP address allows this virtual machine to be reachable 
from the Internet.
So let's say that you want to go ahead and connect to this virtual machine. will actually connect to this virtual
machine via the public IP address

Monitoring
 Boot diagnostics - on
 Diagnostic storage account - 

Advanced
Tags
Review - create

--
view all resources
=======
Connect RDP
 - Download RDP File

---
In vm
Server Manger - dashboard
 Add roles and features
    installation type: role-based or feature-based installation
    server selection: select a server from the server pool
    server roles: web server(iis)
       next , install

localhost

---
ip public don´t working, for network security group
go onto the Networking sections
  add inbound security rule
      destination port ranges: 80
      protocol TCP

ip public refresh
============
Publishing an application from Visual Studio to a VM
vs:
publish
 azure virtual machines

azure portal:
 dns name - configure

configuration
 assigment: static
 DNS name label (demovm)

networking
 add inbound security rule
 port 8172
 TCP

vm:
Add roles and Features Wizard
  Web Server ISS
    Application Development
       ASP NET 4.7 - check
  Mangement tools
    ISS Management Tools
       Management Service - check
install

tools
 iis manager
   vm(demovm)
     management service
       enable remote connections  
     -apply
     -start

local server
  IE Enhanced Security Configuration: OFF

search in internet explorer
  Download Web Deploy v3.5

go again to vs
publish
 azure virtual machines - browse - select vm in azure portal
 edit - validate connection
 publish

Overview:
  1. Assign static IP and DNS name to Virtual Machine
  2. Open Port 8172 in Network Security Groups
  3. In Web Server Roles add
              Management Service and ASP.Net 4.7
  4. Install Web Deploy v 3.5
 
================================
Create linux vm 
port 22

connect with Putty
// This command is used to update the package index
sudo apt-get update

// This command will install the nginx web server
sudo apt-get install nginx

vm:
  Add inbound port rule
     Source: Service Tag
     Destination port range: 80
     Protocol: TCP

test ip public
-----
Accelerated networking
 Accelerated networking enables single root I/O virtualization (SR-IOV) to a VM, greatly improving its networking performance. This high-performance path bypasses the host from the data path, which reduces latency, jitter, 
 and CPU utilization for the most demanding network workloads on supported VM types. The following diagram illustrates how two VMs communicate with and without accelerated networking:
https://docs.microsoft.com/en-us/azure/virtual-network/create-vm-accelerated-networking-powershell

=======================
Generalizing a Virtual Machine  -generate image of vm

Access to vm windows:
System32/Sysprep
   check Generalize
   Shutdown Options: Shutdown 

cloud shell
  deallocated - stop vm
  Stop-AzVM -ResourceGroupName azuredemo -Name demovm -Force

Azure Portal
  vm - overview
  option Capture
    create

Images
Select image - Create new vm
   open port 80 http
   boot diagnostics off

test ip public
================================================
Azure Resource Manager Template
This provides the obility to define your infrastructure as code
You create templates using JSON
This defines this infrastructure and configuration of the resources that need to be deployed

ej: create vm, storage account, virtual network

Resources: resources that need to be deployed
Variable: values that can be reused in the template
Parameters: can be used to provide values during the deployment phase
Outputs: values from the deployed serources

Azure portal
Template deployment - create
  Deploy a simple windows VM
     edit - to view json

-------------------------
Azure backup for vm
  use: Recovery Service vault     [Recovery Point]
       Backup Policy              [File Recovery, VM Recovery, Disk Recovery]
 
in vm
operations
 backup 
  create o edit a new policy
      backup policy
    enable backup
 
 backup now

Recovery Services Vault
  Backup items


But remember, this is pointing to our restore point.
It's not pointing to our virtual machine.
Just keep this in mind. so you can actually now go onto the drives and you can start recovering the files on a file by file basis.


===============================
Docker 
test in vm linux
sudo apt update
sudo apt install apt-transport-https ca-certificates curl software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable"
sudo apt update
sudo apt install docker-ce

sudo docker pull nginx:1.17.0
sudo docker images

sudo docker run --name sampleapp -p 80:80 -d nginx:1.17.0


add rule port 80 in vm

test ip public
------
deploy .net core in vm linux
can use linux with Nginx
        linux with Apache

sudo apt-get update
sudo apt-get install nginx

azure portal:
 add port 80


wget https://packages.microsoft.com/config/ubuntu/18.04/packages-microsoft-prod.deb -O packages-microsoft-prod.deb

sudo dpkg -i packages-microsoft-prod.deb

sudo apt-get update

sudo apt-get install dotnet-sdk-3.1


*Use WinSCP to transfer files - folder publish 

donet coreproj.dll

cd /etc/nginx/sites-available
sudo chmod 667

nano default
location / {
		# First attempt to serve request as file, then
		# as directory, then fall back to displaying a 404.
		proxy_pass http://localhost:5000;
		proxy_http_version 1.1;
		proxy_set_header Upgrade $http_upgrade;
		proxy_set_header Connection keep-alive;
		proxy_set_header Host $host;
		proxy_cache_bypass $http_upgrade;	
	}

sudo nginx -s reload

*cd var/www
*sudo chmod 667 html
*copy files publish\wwwroot\(css,js,lib,favicon.ico)  to nginx var/www/html

------
Containerizing

Dockerfile:
FROM mcr.microsoft.com/dotnet/core/sdk:3.1
WORKDIR /app
COPY  . .
ENV ASPNETCORE_URLS http://*:5000
EXPOSE 5000
ENTRYPOINT ["dotnet", "coreproj.dll"]


sudo docker build -t dotnetapp .
sudo docker run -d -p 5000:5000 dotnetapp

test ip public
------
ACR - Azure Container Registry
Azure Portal:
  Create Container Resgistry

vm linux:

curl -sL https://packages.microsoft.com/keys/microsoft.asc | \
    gpg --dearmor | \
    sudo tee /etc/apt/trusted.gpg.d/microsoft.asc.gpg > /dev/null

AZ_REPO=$(lsb_release -cs)
echo "deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ $AZ_REPO main" | \
    sudo tee /etc/apt/sources.list.d/azure-cli.list

sudo apt-get update

sudo apt-get install azure-cli

sudo az login

sudo az acr login --name appregistry2020

sudo docker tag dotnetapp appregistry2020.azurecr.io/dotnetapp

sudo docker push appregistry2020.azurecr.io/dotnetapp



-----
Azure Container Instances
Use Azure Container registry

port 5000
test ip public

----------------
---------------
Kubernetes
This is an open-source platform that is used to magning containerized workloads
Kubernetes is able to provide a DNS name to your containe


create cluster
you can use the interface or commands

cloud shell, powershell
// Here we will create a new resource group
az group create --name kubernetesgrp --location eastus

// Then we will create a new Kubernetes cluster
az aks create --resource-group kubernetesgrp --name democluster --node-count 1 --enable-addons monitoring --generate-ssh-keys

// We will then install the kubectl tool
az aks install-cli --install-location=./kubectl

// This allows kubectl to connect to the Kubernetes cluster
az aks get-credentials --resource-group kubernetesgrp --name democluster 

-------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-app
  template:
    metadata:
      labels:
        app: nginx-app
    spec:
      containers:
      - name: nginx
        image: nginx:1.17.0
        ports:
        - containerPort: 80
----
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: nginx-app

kubectl apply -f app.yml

kubectl apply -f service.yml

kubectl get service nginx-service --watch

kubectl delete services nginx-service

---------------------
ACR - AKS
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-image
        image: appregistry6000.azurecr.io/dotnetapp
        ports:
        - containerPort: 5000

apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: LoadBalancer
  ports:
  - port: 5000
  selector:
    app: my-app

kubectl delete services nginx-service

kubectl delete deployment nginx-deployment

$AKS_RESOURCE_GROUP="kubernetesgrp"
$AKS_CLUSTER_NAME="democluster"
$ACR_RESOURCE_GROUP="demogrp1"
$ACR_NAME="appregistry6000"

//Use Service Principal
$CLIENT_ID=$(az aks show --resource-group $AKS_RESOURCE_GROUP --name $AKS_CLUSTER_NAME --query "servicePrincipalProfile.clientId" --output tsv)

$ACR_ID=$(az acr show --name $ACR_NAME --resource-group $ACR_RESOURCE_GROUP --query "id" --output tsv)

az role assignment create --assignee $CLIENT_ID --role acrpull --scope $ACR_ID

kubectl apply -f app.yml

kubectl apply -f service.yml

kubectl get service my-service --watch

=============================================
=============================================

Azure App Service (Saas)

App Service Plan
Free 60min per day
Shared 240 per day
https://docs.microsoft.com/en-us/azure/app-service/overview-hosting-plans
Pricing tier (Free, Shared, Basic, Standard, Premium, PremiumV2, PremiumV3, Isolated)

---
Azure Portal 
Create Web App 
  Runtime Stack: ASP NET V4.7
  SO: Windows
 
 App Service Plan: Basic B1
 Enable Application Insights -no

VS:
 Publish
   App Service: Select existing
--------------
--------------
Azure Web App - Linux
   Runtime Stack: .net Core 3.1(LTS)
    SO: Windows

 App Service Plan: Standard1
  

VS:
 Publish
   App Service: Select existing

-------------
-------------
App Service
  Overview - Stop, Restart

Configuration
  Application Settings
  Connection Strings

Authentication/Authorization
   App Service Authentication - on
      AD
   Backup

   Scale up(App Service plan)
   Scale out(App Service plan)

   Networking
      VNet integration
      Hybrid connections
      Azure front door with web application firewall
      Azure CDN

---------
Web App - Docker container
Create web app
  Publish: Docker Container
  linux
  acr

----
App services logs
  Application logging
  Web server logging
  Detailed Error messages
  failed request tracing
  deployment logging

#note: use project- mvcapp
vs
publish project un app service

Monitoring
  App service logs
  level information
  webserverloggin: file system


azure poral -app service
deployment center 
  FTP - User Credentials
       username, pw

use WinSCP
  see logs

-------------------------
web app - publish from github
Github Extension for Visual Studio
login to github in vs

Add solution to source control
commit
commit stage
sync
publish to github

app service
 deployment center
   github 
     github actions

vs
other commit
push

refresh page

az appservice plan create --name "demoappplan2020" --resource-group "azuredemo" --sku Free
az webapp create --name "demoapp2050" --resource-group "azuredemo" --plan "demoappplan2020"
az webapp deployment source config --name "demoapp2050" --resource-group "azuredemo" --repo-url "https://github.com/alashro/WebAppNew" --branch master --manual-integration


-------------
azure cli
azure cloud shell
$gitrepo="https://github.com/alashro/demoapp1000"
az group create --location centralus --name staging-grp
az appservice plan create --name newappplan2000 --resource-group staging-grp --sku B1

az webapp create --name newapp2000 --resource-group staging-grp --plan newappplan2000 
az webapp deployment source config --name newapp2000 --resource-group staging-grp --repo-url $gitrepo --branch master --manual-integration

-------
Custom Domains
//godaddy
edit in provider domain (example:godaddy)
app service
Custom Domains
  copy IP address
  
  add custom domains
-------
SSL
TLS/SSL Settings
   Create App
   Service Managed Cerificate
Custom Domains
   Add binding
      choose certificate
      TLS/SSL: SNI SSL
-------
CORS: Cross-Origin Resource Sharing
Publish project webapi vs
Publish
  new - select existing
  publish

project consumer api
consume.html - ajax
Publish
  new - select existing
  publish

app service -webapi
  CORS
    enable
    allow origin: consumeapi.azurewebsite.net
-------
deployment slots
publish vs
scale up - production s1
deployment slots - add slot
swap

------
deployment slots power shell
//create variables that can be used in the PowerShell script. Here we are setting the location for the web application, 
the name of a new resource group and the name of the new Azure Web App
$location="Central US"
$resourcegrp="newgrp"
$webappname="demoapp4040"

// Next we issue the command to create a new resource group
New-AzResourceGroup -Name $resourcegrp -Location $location

// Next we issue the command to create a new App Service Plan
New-AzAppServicePlan -Name $webappname -Location $location -ResourceGroupName $resourcegrp -Tier Standard

// Next we issue the command to create a new Web App
New-AzWebApp -Name $webappname -Location $location -ResourceGroupName $resourcegrp -AppServicePlan $webappname

// Next we issue the command to create a new Web App deployment slot
New-AzWebAppSlot -Name $webappname -ResourceGroupName $resourcegrp -Slot "staging"

-----------------------
----------------------
.NetCore azure web app linux
cloud shell 
az group create -name linuxgroup --location "West Europe"
az appservice plan create --name demoplan --resource-group linuxgroup --sku B1 -ls-linux
az webapp create --resource-group linuxgroup --plan demoplan --name linuxapp2020 -- "DOTNETCORE|2.2" --deployment-local-git


https://dotnet.microsoft.com/download
cmd local
donet --version
mkdir coreapp
cd coreapp
dotnet new web
dotnet run

git add .
git commit -m ""

cloud shell
az webapp deployment user set --user-name demousr2020 --password DemoUsr@2020

git remote add azure https://demousr2020@linuxapp2020.scm.azurewebsites.net/linuxapp2020.git
git push azure master

=================================================
=================================================
Azure Functions

Function App
Subs
 Resource Group

Function App name
Publish: code
Runtime stack .net core
version: 3.1
region: Centrar US
--
hosting
 storage account
 plan type: consumption (serverless)
--
enable application insights
create
=====
Functions - add
   HTTP Trigger
  
code + test
using System.Net;
using Microsoft.AspNetCore.Mvc;
using Microsoft.Extensions.Primitives;

public static async Task<IActionResult> Run(HttpRequest req, ILogger log)
{
	log.LogInformation("My first function");
	return (ActionResult)new OkObjectResult("Azure Function");
}


save
Test
  http method get
  run

Get function url
//test url in browser
----------------
Options:
function.json
run.csx

app files
  host.json  - This configuration file affect all of the functions which are part of your function app.


add function - httptrigger
 test
  query

 test
  body

monitor
//results mey be deplayed for up to 5minutes.

code+test
   get url

test in postman
query params
name angel

body
{"name":"Angel"}

--------------------------
#r "Newtonsoft.Json"

using System.Net;
using Microsoft.AspNetCore.Mvc;
using Microsoft.Extensions.Primitives;
using Newtonsoft.Json;

public static async Task<IActionResult> Run(HttpRequest req, ILogger log)
{
    log.LogInformation("C# HTTP trigger function processed a request.");

    string requestBody = await new StreamReader(req.Body).ReadToEndAsync();
    Course obj;
    obj = JsonConvert.DeserializeObject<Course>(requestBody);
    log.LogInformation((obj.id).ToString());
    log.LogInformation(obj.name);
    log.LogInformation((obj.rating).ToString());

    return (ActionResult)new OkObjectResult(obj);
}

public class Course
{
    public int id {get;set;}
    public string name {get;set;}
    public double rating;
}


code+test
 test
   body
    {"id":1,"name":"Angel","rating":5.0}
------
Timer trigger
CRON Schedule
test

---
Durable Functions
state
Trigger Function -> Orchestrator Function -> Activity Function
--
create durable function
 Durable Functions HTTP starter

//invoke orchestrator function
create Durable Functions Orchestrator

create Durable Functions activity


get url function starter
postman post - url  {functionName}=Orchestrator

answer
uri's - select first
=Orchestrator has gone ahead ans run that activity function three times

-----------
VSCode Azure Functions
Extension: Azure Functions, Azure Account, C#
view - Command Palette
          Azure: Sign In
create azure functions

Run - Start Debugging

localhost:7071/api/visual_studio_code?name=Angel

Deploy to Function App
  Subscriptions
    select or create function app
-------------
VS Azure Function 
Create a new project
   Azure Functions
      Http Trigger

run
localhost:7071/api/Function1?name=Angel

===================================
Storage Account
Service:
blob, table, file, queue
Blob: store massive amounts of unstructured data on the cloud
      image, documents,video and audio files
  Block blobs - storing text and binaty data
  Append blobs - ideal logging data
  Page blobs - used to store virtual hard disk files for AZ vm

Azure Storage Account - Replication
Locally-redundant-storage(LRS)
Zone-redundant storage (ZRS)
Geo-redundant storage (GRS)
Read access Geo-redundant storage(RA-GRS)

hot
cold
------
create storage account
subs
  resouce group
name
location
performance - standard
account kind - storagev2
replication: locally-redundant storage (LRS)
access tier: hot

advanced:
#blob public access - enabled   // to production in disabled

--------
Container
all blob is part a container
create new container in storage account
  
  upload file
  copy url - //test (resource not found)
  Change access level: Blob(anonymous read access for blobs only)
  //test again url

--------
Storage Explorer
install tool Azure Storage Explorer
https://azure.microsoft.com/es-es/features/storage-explorer/
manage account - add an azure account
   apply
--------
Access Keys
2 keys
tool storage explorer:
   Use a storage account  name and key
            Display name
            Account name
            Account key
--------
Storage Account - Azure cli
az login
az storage account create --name stagingstore4000 --resource-group demogrp1 --location EastUS --sku Standard_LRS 
az storage container create --account-name stagingstore4000 --name demo

//local directory have a file - sample.txt 
az storage blob upload --account-name stagingstore4000 --container-name demo --name sample.txt --file sample.txt

az storage container set-permission --account-name stagingstore4000 --name demo --public-access blob
az storage blob list --account-name stagingstore4000 --container-name demo --output table
az storage blob download --account-name stagingstore4000 --container-name demo --name sample.txt --file sample.txt
--------

AzCopy tool
https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10

//tenant-id in azure directory
azcopy login --tenant-id=5f5f1c90-abac-4ebe-88d7-0f3d121f967e

//create container in storage account
azcopy make "https://stagingstore4000.blob.core.windows.net/demo"


Access control(IAM)
  add
    role: Storage Blob Data Contributor
    select: user

//Copy
azcopy copy "sample.txt" "https://stagingstore4000.blob.core.windows.net/demo/sample.txt"
//Download
azcopy copy "https://stagingstore4000.blob.core.windows.net/demo/sample.txt" "sample.txt"

------
https://docs.microsoft.com/en-us/dotnet/api/overview/azure/storage?view=azure-dotnet
vs - azure blob storage - v12
Tools
  NuGet Package Manager
       Manage Nuget Packages for Solution

azure portal
  storagea account - acess key
        - connection string


1) BlobServiceClient Class - This is used as a logical representation of the Azure Storage account and provdies access to work with the Blob service.
You can use the Constructor of the BlobServiceClient class to pass in the connection string to connect to the Azure Storage account.
You then have methods such as

a) CreateBlobContainer and CreateBlobContainerAsync- This can be used to create a container
b) GetBlobContainerClient - This returns a reference to an existing container


2) BlobContainerClient - This is used as a logical representation of the container service in the Azure Storage account
You then have methods such as

a) GetBlobClient - This is used to get a logical representation to a blob
b) GetBlobsAsync - This is used to return a list of blobs in a container. Each blob is returned as an object of the class BlobItem

3) BlobClient - This class is used to work with Azure Storage Blobs
You then have methods such as

a) Upload and UploadAsync - This can be used to upload a blob onto the container
b) Download and DownloadAsync - This can be used to download a blob from the container

4) BlobItem - This class is used to represent a Blob in a container.

For the BlobItem class , you have properties such as Metadata, Name , Properties and Snapshot
------
vs - azure blob storage - v11
Tools
  NuGet Package Manager
       Manage Nuget Pavkages for Solution
------
Azure blob properties and metadata
select file in azure portal (sample.txt)
Tier - hot
lenght
key          - value
deparment   - IT


vs blobproperties
run GetMetadata()
SetMetadata()
-------
Shared Access Signatures
private access
select file
  Generate SAS
    Permissions - READ
    Start and expiry date/time
    Generate SAS token and URL

storage account
   shared access signature
    Generate SAS and connection string

--------
Shared Access Signature - Net
generate sas
---------
Access Tiers
Storage Account
  Configuration - Access tier(default)
select file
  change tier - cold, 
         Archive - blob that are not accessed for a long duration of time

----------
Blob snapshots
select file
  Edit file
  View snapshot
     Promote snapshot - go back previous version
----------
Blob lease
Establish a lock on the blob for write and delete operations

select file
   Acquire lease
----------
Storage Account - Soft Delete
Data Protection
  Blob soft delete: Enabled
  Retention polices: period 7 days

select file
   Create 2 Snapshot 
delete file -  check also delete blob snapshots

container
  show deleted blobs
      undeleted
----------
ARM Templates Storage Accounts
Azure Portal
Template deployment
  Build your own template in the editor
     //Create storage account multiple times
{
    "$schema": "https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#",
    "contentVersion": "1.0.0.0",
    "parameters": {
        "storageCount": {
            "type": "int",
            "defaultValue": 2
        }
    },
    "resources": [
        {
            "type": "Microsoft.Storage/storageAccounts",
            "apiVersion": "2019-04-01",
            "name": "[concat(copyIndex(),'storage80808')]",
            "location": "[resourceGroup().location]",
            "sku": {
                "name": "Standard_LRS"
            },
            "kind": "StorageV2",
            "properties": {},
            "copy": {
                "name": "storagecopy",
                "count": "[parameters('storageCount')]"
            }
        }
    ],
    "outputs": {}
}

   Save
     resource group
--------
https://docs.microsoft.com/en-us/azure/storage/common/redundancy-migration?tabs=portal
1) Upgrading a storage account

If you have a Blob only storage account or a general purpose v1 storage account, you can easily migrate to a General Purpose v2 storage account. The general purpose v2 storage account provides lowest per-gigabyte capacity prices for Azure Storage and has much more features
If you do have such an account, you can just go to the configuration and click on Upgrade
       
2) Changing the replication type for a storage account

If you need to migrate your storage account from Locally redundant storage or Geo-redundant storage  to Zone redundant storage in the primary region with no application downtime, you can request a live migration from Microsoft. During a live migration, you can access data in your storage account, and with no loss of durability or availability. The Azure Storage SLA is maintained during the migration process. There is no data loss associated with a live migration. Service endpoints, access keys, shared access signatures, and other account options remain unchanged after the migration.
====================
====================
Azure Table Storage
Used to store NoSQL Data
 key/attribute

table
 entity [set properties]  name-value pair
   partition key, row key and timestamp

----
storage account
table
  add

storage explorer
  add entity
    
shared access signature
  check only table
    generate SAS and connection string

test postman url - GET
add in url Customer
windows-/Customer(PartitionKey=Architect.RowkEY='userA')?sv=
---
Partition and Row key
Partition Key - These are string values. These define the partition the entity is stored in.
Row Key - These are string values. These are used to uniquely identity entities within each partition
TimeStamp - Tells the last time the entity was modified.
The Table Primary Key is the combination of the Partition and Row Key
-----
vs .net yable storage
vs - azure cosmos table - v1.0.7
Tools
  NuGet Package Manager
       Manage Nuget Packages for Solution

Storage Account - key: connection string
-------
Queue Storage
add queue
  add message to queue
------
Queue Storage - VS  - 11.1.5
azure portal
name=appqueue - storage account
copy key - connection string
vs
Tools
  NuGet Package Manager
       Manage Nuget Packages for Solution
           install: Microsoft.Azure.Storage.Queue

//use FetchAttributes(); before to get

project: send, receive

============
Azure Functions - Queue binding

add
  Azure Queue Storage Trigger
      newfunction(name): queue-trigger 
      Queue name: appqueue
      Storage account connection: <select storageaccount>

code+test

//JObject: json object


#r "Newtonsoft.Json"


using System;
using Newtonsoft.Json.Linq;

public static void Run(JObject myQueueItem, ILogger log)
{
       Course coursebj=myQueueItem.ToObject<Course>();

       log.LogInformation((coursebj.Id).ToString());
       log.LogInformation(coursebj.Name);
       log.LogInformation((coursebj.rating).ToString());

}

public class Course
{
    public int Id;
    public string Name;
    public double rating;
}


add message in queue
{ "Id" : 1, "Name" : "AZ-204", "rating" : 4.5 }

queue-trigger go on to the monitor section
-----------------
Azure Functions - Queue and Table binding
create table
  name: customer

code-test
#r "Newtonsoft.Json"


using System;
using Newtonsoft.Json.Linq;

public static Course Run(JObject myQueueItem, ILogger log)
{
       Course courseobj= new Course();
       courseobj.PartitionKey=myQueueItem["Id"].ToString();
       courseobj.RowKey=myQueueItem["Name"].ToString();
       courseobj.rating=myQueueItem["rating"].Value<double>();
        return courseobj;
}

public class Course
{
    public string PartitionKey {get;set;}
    public string RowKey {get;set;}
    public double rating {get;set;}
}


function .json:
{
  "bindings": [
    {
      "direction": "in",
      "type": "queueTrigger",
      "connection": "demotore4000_STORAGE",
      "name": "queueitem",
      "queueName": "appqueue"
    },
    {
      "type": "table",
      "name": "$return",
      "tableName": "customer",
      "connection": "demotore4000_STORAGE",
      "direction": "out"
    }
  ],
  "disabled": false
}

add message in queue
{ "Id" : 1, "Name" : "AZ-204", "rating" : 4.5 }
refresh and see the table
------------
Azure Functions - Multiple Output bindings

output table,queue, blob
table: newCustomer
queue: appqueue

local.settings.json - connection

use program to send messagees to queue
=========================
Azure SQL Database
IaaS   -        PaaS  -  [Elastic Pool, Single Database, Managed Instance]
ip private      firewall
                 purchasing model [DTU Database Transaction Unit, vCore-based]

sql database
resource
database name
server
  create new
compute + storage
  configure database
      basic

connectivity method
  public endpoint
firewall rules
   Add current IP address - Yes
-----
copy sever name
Download SQL Server Management Studio (SSMS)
https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-ver15

use server name
  user
  pass
------
azure web app - azure sql database - vs

azure portal:
show connection string db 
    - replace password

vs:
page connecion string in webconfig 
<connectionStrings>
    <add name="CourseContext" providerName="System.Data.SqlClient" connectionString="" />
  </connectionStrings>


sql server:
insert into Courses(name,rating) values('AZ-104',4.4)
insert into Courses(name,rating) values('AZ-300',4.5)
insert into Courses(name,rating) values('AZ-301',4.6)

vs - run
localhost

azure portal:
set server firewall
 cliente ip address

vs
publish app service
create new registre

select * from Courses - view data
-------
azure web app - connetion string
vs
<connectionStrings>
    <add name="connstring" connectionString="Server=tcp:demoserver4000.database.windows.net,1433;Initial Catalog=appdb;Persist Security Info=False;User ID=demousr;Password=123;MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;" providerName="System.Data.SqlClient"/>
  </connectionStrings>

azure portal - app service
configuration
 add connection string
   value:  Server=tcp:demoserver4000.database.windows.net,1433;Initial Catalog=appdb;Persist Security Info=False;User ID=demousr;Password=123;MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;" providerName="System.Data.SqlClient
   type: SQLAzure
   name: connstring	


vs -Model
 database.cs
    connstring = ConfigurationManager.ConnectionStrings["connstring"].ConnectionString;

publish app service


--------
AzureFunction - Azure SQL Database
azure portal - sql database
                 query editor: select * from[dbo].[Products]
vs - azurefunction - httptrigger
Function1.cs
  //string _conn_string = "Server=tcp:demoserver4000.database.windows.net,1433;Initial Catalog=demodb;Persist Security Info=False;User ID=demousr;Password=123;MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;";
    string _conn_string = Environment.GetEnvironmentVariable("sql_connection");

run localhost

publish azure function

azure portal - function app
configuration
  new application settings
     Name:sql_connection
     Value: Server=tcp:demoserver4000.database.windows.net,1433;Initial Catalog=demodb;Persist Security Info=False;User ID=demousr;Password=123;MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;

function
  get function url
  test in browser
---------
Azure Container Intances - Container Groups
//you can deploy multiple containers as part of a container
deployment yaml

apiVersion: 2020-10-01
location: eastus
name: container
properties:
  containers:
  - name: nginx-demo
    properties:
      image: nginx
      resources:
        requests:
          cpu: 1
          memoryInGb: 1.5
      ports:
      - port: 80
  osType: Linux
  ipAddress:
    type: Public
    ports:
    - protocol: tcp
      port: 80
tags: null
type: Microsoft.ContainerInstance/containerGroups

azure cloud shell - powershell
upload deployment.yaml

az container create --resource-group demogrp1 --file deployment.yaml
test public ip 

----------
Azure Container Intances - Container Groups - Container registry
vs 
 Program.cs
  create blob
   memory stream - enter the string into blob

run program

azure portal - storage account
container

donet publish

WinSCP
copy Dockerfile to linux machine
copy files from folder publish

Dockerfile
FROM mcr.microsoft.com/donet/core/sdk:3.1
WORKDIR /app
COPY . .
ENTRYPOINT["donet", "blobproject.dll"]

Container registry
  access key 
    -Adminuser: enable 

linux vm
 docker, azure cli
sudo az login
sudo az acr login --name demoregistry3000
sudo docker build -t donetapp .
sudo docker tag dotnetapp demoregistry3000.azurecr.io/dotnetapp
sudo docker push demoregistry3000.azurecr.io/dotnetapp

Container registry
  repositories


apiVersion: 2018-10-01
location: eastus
name: mycontainer
properties:
  containers:
  - name: dotnetapp
    properties:
      image: demoregistry3000.azurecr.io/dotnetapp
      resources:
        requests:
          cpu: 1
          memoryInGb: 1.5
  osType: Linux
  imageRegistryCredentials:
    - server: demoregistry3000.azurecr.io
      username: demoregistry3000
      password: RP+e8rd6ZDGMK2cKZODtFhNtM8jFm8im
tags: null
type: Microsoft.ContainerInstance/containerGroups


azure cloud shell - powershell
upload deployment.yaml
ls
az container create --resource-group demogrp1 --file deployment.yaml

//this file has been created out of the container, which is running in the container instance

--------
Azure Container Instances - Secrets
storage account 
    - keys-connection string

encode connection string
powershell
$value= "DefaultEndpointsProtocol=https;AccountName=demostore40000;AccountKey=oKSUsm+5HsyIVxORQAZoWqYvxOVR81F3ZCowrNe/gUvVl409q8E2TMQtZlycuN9fI/OxVBu/qKDhNNSQd09LZw==;EndpointSuffix=core.windows.net"
$Bytes = [System.Text.Encoding]::UTF8.GetBytes($value)
$Base64 = [Convert]::ToBase64String($Bytes)
Write-Host $Base64

vs
string mountpath = "/mounts/secrets";
            
// Will get the directory - volumesecret
var folders = Directory.GetDirectories(mountpath);
foreach(var folder in folders)
{
     Console.WriteLine($"Folder : {folder.ToString()}");
     var AllFiles = Directory.GetFiles(folder);
     foreach(var file in AllFiles)
     {
           storageconnstring = File.ReadAllText(file);
           Console.WriteLine(storageconnstring);
     }                   
}
 donet publish

copy files to vm from folder publish

sudo docker rm --force <id>
sudo docker build -t dotnetapp .
sudo docker tag dotnetapp demoregistry3000.azurecr.io/donetapp
sudo docker push demoregistry3000.azurecr.io/donetapp

rm deployment.yml
upload file

deployment.yml
apiVersion: '2018-10-01'
location: eastus
name: mycontainernew
properties:
  containers:
  - name: dotnetappnew
    properties:
      image: demoregistry3000.azurecr.io/dotnetapp
      resources:
        requests:
          cpu: 1
          memoryInGb: 1.5
      volumeMounts:
      - mountPath: /mounts/secrets
        name: volumesecret
  osType: Linux
  imageRegistryCredentials:
    - server: demoregistry3000.azurecr.io
      username: demoregistry3000
      password: RP+e8rd6ZDGMK2cKZODtFhNtM8jFm8im
  volumes:
  - name: volumesecret
    secret:
      storage-connection: RGVmYXVsdEVuZHBvaW50c1Byb3RvY29sPWh0dHBzO0FjY291bnROYW1lPWRlbW9zdG9yZTQwMDAwO0FjY291bnRLZXk9b0tTVXNtKzVIc3lJVnhPUlFBWm9XcVl2eE9WUjgxRjNaQ293ck5lL2dVdlZsNDA5cThFMlRNUXRabHljdU45ZkkvT3hWQnUvcUtEaE5OU1FkMDlMWnc9PTtFbmRwb2ludFN1ZmZpeD1jb3JlLndpbmRvd3MubmV0
tags: null
type: Microsoft.ContainerInstance/containerGroups

Container intances
  Containers
      logs


====================
Cosmos DB
Database account, Database, Container  , Item
-----------------------------------------------
SQL API		  Database, Container  , Document
Table API	  NA	  , Table      , Item
MongoDB API	  Database, Collection ,  Document
Cassandra API     Keyspace, Table      , Row
Gremlim API       Database, Graph      , Node or edge

Create Cosmos DB
Subscription
 resource group

Account name
API Core(SQL)
Apply free Tier Discount

----
azure cosmos db account
Data Explorer
 Add Container
   database id: appdb
   throughput 
   container id: customer
   partition key: /city

//full screen
new item
{
  "customerid": 1,
  "customername": "Strevens",
  "city":  "Bogota"
}

{
  "customerid": 1,
  "customername" : "Jane",
  "city" : "Miami"
}


{
  "customerid": 2,
  "customername" : "Jun",
  "city" : "New York"
}


{
  "customerid": 3,
  "customername" : "Mika",
  "city" : "Florida"
}


{
  "customerid": 4,
  "customername" : "Leo",
  "city" : "New York"
}

sql query
select * from c where c.city="New York"

select * from c where c.customername="Mika"


----
id property, generate with azure portal

generate the ID yourself
{
  "id": 5,
  "customername" : "Luke",
  "city" : "Miami"
}
----
Your RU's get split across the partitions
miami, newyork, florida
200     200      200    -  RU:600

----
vs
Tools
  Nuget Package Manger
    Manage Nuget Package for Solution
      Microsoft.Azure.Cosmos  3.8.0

customer.cs
   map onto the items that i'm storing in container
program.cs
//CreateNewItem().wait();
//ReadItem().wait();
//ReplaceItem().wait();
//DeleteItem().wait();


azure cosmos db account
keys
  URI - endpoint
  PRIMARY OR SECONDARY KEY - key

---------
embedding data

{
  "customerid": 1,
  "customername" : "John",
  "city" : "Miami"
}


{
  "customerid": 2,
  "customername" : "Jane",
  "city" : "New York",
  "orders" :[
  {
	"orderid" : 1000,
	"productid" : 100,
	"quantity" : 2
  }]
}


{
  "customerid": 3,
  "customername" : "Michael",
  "city" : "Florida",
  "orders" :[
  {
	"orderid" : 1001,
	"productid" : 200,
	"quantity" : 2
  }]

}


{
  "customerid": 4,
  "customername" : "David",
  "city" : "New York"
}

select * from customer.orders c
select * from c in customer.orders where c.orderid=1000
select count(c) from c in customer.orders

----
vs 
CreateNewItem().Wait();
ReadItem().Wait();
run

---
referencing data
---
Change Feed - The listen to all changes in a container and outputs a stored
list of documents that changed in the order in which the were modified

azure function 
  cosmos db trigger
     NewFunction: apptrigger
     Cosmos DB account connection:
             new - database account
     Database name: appdb
     Collection name: customer
 
   
code + test:

#r "Microsoft.Azure.DocumentDB.Core"
using System;
using System.Collections.Generic;
using Microsoft.Azure.Documents;

public static void Run(IReadOnlyList<Document> input, ILogger log)
{
    if (input != null && input.Count > 0)
    {
        log.LogInformation("Documents modified " + input.Count);
                foreach(var customer_obj in input )
                {
                       int customer_id=customer_obj.GetPropertyValue<int>("customerid") ;
                       string customer_name=customer_obj.GetPropertyValue<string>("customername") ;
                       string customer_city=customer_obj.GetPropertyValue<string>("city") ;

                       log.LogInformation($"The customer id is {customer_id}");
                       log.LogInformation($"The customer name is {customer_name}");
                       log.LogInformation($"The customer city is {customer_city}");
                }
    }
}


add new item in cosmosdb
{
  "":3
  "":"",
  "":""
}

log in azure function

--------
create new stored procedure

stored procesure id : demo
store procedure body: (is in javascript)

function Sample() {
        var context = getContext();
        var response = context.getResponse();
        response.setBody("Executing a Stored Procedure");
}

run vs

--
stored procesure id : demo
store procedure body: CreateItem
function AddItem(p_document) {

    var context = getContext();
    var container = context.getCollection();

    var accepted = container.createDocument(container.getSelfLink(),
        p_document,
        function (err, item) {
            if (err) throw new Error('Error' + err.message);
            context.getResponse().setBody(item.id)
        });
    if (!accepted) return;
}

run vs

-------------
triggers
create a new trigger
Trigger Id: AppendDocument
Trigger Type: Pre
Trigger Operation: Aññ
Trigger Body:
function Append(){
	var context=getContext();
	var request=context.getRequest();
	var document=request.getBody();

	document["orders"]=0;
	request.setBody(document);
}
========================
Cosmos DB Local Emulator
https://docs.microsoft.com/en-us/azure/cosmos-db/local-emulator-release-notes
--
time to live
settings - on

"ttl": 1000
--
indexing
settings
  index policy

{
    "indexingMode": "consistent",
    "automatic": true,
    "includedPaths": [
        {
            "path": "/*"
        }
    ],
    "excludedPaths": [
        {
            "path": "/\"_etag\"/?"
        }
    ],
    "compositeIndexes": [
        [
            {
                "path": "/city",
                "order": "ascending"
            },
            {
                "path": "/customername",
                "order": "ascending"
            }
        ]
    ]
}

SELECT * FROM c order by c.city, c.customername asc

---
diagnostic data
 Query Stats

vs
jvalue  - Newtonsoft.Json
run
-----
cosmosdb
replicate data globally
 save

vs run

----
Consistency levels
azure cosmos db
default consistency

1) Strong - Here the reads will be guaranteed to give the most recent committed version of an item. 
Where can you see a use case of this. Let's say you have an application used by a hospital. 
Let' say that the hospital has multiple locations. 
If the hospital always to get the latest version always for a patient's records at any point in time, 
they might need Strong consistency. Since this is critical , it is important to always have the most recent information.

2) Bounded staleness - Here the reads can lag behind the writes by at most "K" versions of an item or by "T" time interval.

3) Session - This guarantees consistency within a particular session

4) Consistent prefix - Here the client will never see an out of order write

5) Eventual - This is the least form of consistency wherein the client will not be guaranteed on the order of the writes

---------
cli azure cosmos db
create a new Cosmos DB account
az cosmosdb create -n newaccount6000 -g demogrp1 --default-consistency-level Session --locations regionName='West US 2' failoverPriority=0 isZoneRedundant=False

create an Azure Cosmos DB account with multiple regions
az cosmosdb create -n newaccount5000 -g demogrp1 --default-consistency-level Session --locations regionName='West US 2' failoverPriority=0 isZoneRedundant=False --locations regionName='East US 2' failoverPriority=1 isZoneRedundant=False

create an Azure Cosmos DB account with the Table API
az cosmosdb create -n newaccount6000 -g demogrp1 --default-consistency-level Session --locations regionName='West US 2' failoverPriority=0 isZoneRedundant=False --capabilities EnableTable
---------
azure cosmos db -table api
api: Azure Table

vs run
use connection string

NewItem().Wait();
ReadItem().Wait();
UpdateItem().Wait();
DeleteItem().Wait();

====================================
====================================
Azure Security
Azure AD

Azure Portal
Azure Directory
  Users

create user

 Groups
   Collection of user -- access

--
Role Based Access Control

Owner: Grants full access to manage all resources, including the ability to assign roles in Azure RBAC.
Contributor: Grants full access to manage all resources, but does not allow you to assign roles in Azure RBAC, manage assignments in Azure Blueprints, or share image galleries.
Reader: View all resources, but does not allow you to make any changes

----
vm
Acces Control (IAM) 
  Role assigment
    Add role assignment
      Role: Reader
      Assign  access to: Azure AD
      Select: user

login with user
vm
 Networking "You do not have read permission for this network interface"


Resource Group
Acces Control (IAM) 
  Role assigment
    Add role assignment
      Role: Reader
      Assign  access to: Azure AD
      Select: user

Public IP Address
Access Control (IAM)
  Role assigment
     the user have permission

-----
OAuth and OpenID Connect

username
password

security token

ProductsAPI
OrdersAPI
CustomerAPI

Azure AD
User

----
OAuth2.0 Implicit Flow

This is an open standard on how to implement authorization.

- Resource Owner – This is typically the end user.
- Resource Server – This is the server that is hosting a protected resource.
- Client – This is the application that is requesting the use of the protected resource on behalf of the user.
- Authorization Server – This is the entity that authorizes the Resource Owner and issues access tokens.

What are the client types

- Confidential clients – These are applications that are able to securely authenticate with the authorization server. This is done via the use of a client secret.
- Public clients – These are applications that run on the browser or on a mobile device that are unable to use client secrets.
- The Authorization Code Flow

Here the application redirects the user to the Authorization Server.

- The user completes the authorization steps presented by the Authorization Server.
- The user is then redirected back to the application with an authorization code in the query string.
- The application then exchanges the authorization code for the access token.

The different query string parameters

- response_type=code – This tells the authorization server to initiate the authorization code flow.
- client_id – This is the public identifier of the application.
- redirect_uri – This tells the authorization server where to send the user back to after the request has been approved.
- scope – This is one or more space-separated strings that indicate the permissions the application is requesting for.
- state – This is a random string generated in the request. This should then be checked by the application after the user authorizes the application. This helps to prevent Cross Site Request Forgery attacks.

Next the application makes a POST request to the token endpoint to get the token

- grant_type=authorization_code– This tells the token endpoint on the authorization server to use the Authorization Code grant type.
- code – This was the code that was exchanged in the initial redirect request
- client_id – This is the public identifier of the application.
- client_secret – The application’s client secret

The Implicit Grant Flow

Here the application redirects the user to the Authorization Server.
The user completes the authorization steps presented by the Authorization Server.
The user is then redirected back to the application with the access token in the URL fragment.

The different query string parameters

- response_type=token – This tells the authorization server to initiate the implicit flow.
- client_id – This is the public identifier of the application.
- redirect_uri – This tells the authorization server where to send the user back to after the request has been approved.
- scope – This is one or more space-separated strings that indicate the permissions the application is requesting for.
- state – This is a random string generated in the request. This should then be checked by the application after the user authorizes the application. This helps to prevent Cross Site Request Forgery attacks.

The Client Credentials Flow

The Client Credentials Flow is used by clients to obtain an access token outside the context of the user.
The different query string parameters

- grant_type = client_credentials
- scope – This is one or more space-separated strings that indicate the permissions the application is requesting for.
- client_id – This is the public identifier of the application.
- client_secret – The application’s client secret



-----
vs -  nuget package manager
    Microsoft.Identity.Web

Azure AD
*App registrations
  new registrations
  
  Redirect URI: https://localhost:44358/signin-oidc

copy Application (client) ID, TenantID
paste in appsettings.json  "ClientID", "TenantID"

*Certificates & secrets 
  Add a client secret 

copy value secret and paste
"ClienteSecret"  - appsettings.json

*API Permissions

*Authentication
Logout URL
 https://localhost:44358/signout-callback-oidc
 check ID tokens

run vs
-----

Storage account 
add role assigment
 Role:Storage blob Data Contributor

API Permission
 add permission
  Request API permissions 
    - azure storage
        user_impersonation
    - Microsoft Graph

vs run
-----
Multi-factor Authentication
Azure AD Premium P2


Security
 Conditional Access
   new
     name:demo-app-policy
     Users and groups: All users
     Cloud apps or actions: Cloud apps - select apps
     Conditions
     Grant
       Grant access
          - Require multi-factor autentication

    enable policy -on

vs
-------
OpenID Connect
vs
nuget package
  Microsoft.AspNetCore.Authentication

---
azure app service
authentication prociders
  azure active directory

action to take when request is not authenticated
Log in with Azure Active Directory

vs
publish
ressirect to login microsoft - azure AD

----
azure key vault service
create key vault
--
Service Principal - sp
--
create secret
vs

cloud shell - powershell
 az ad sp create-for-rbac -n secretapp --skip-assignment

 az keyvault set-policy --name demovault9090 --spn "c0fcc4c9-9739-45cd-87a3-bd53072de649"
 -- secret-permissions backup delete get list set

vs run
-------
Encription keys
create key

az keyvault set-policy --name demovault9090 --spn "c0fcc4c9-9739-45cd-87a3-bd53072de649"
 -- secret-permissions backup delete get list create encript decrypt update


-----
key vault
Access polices
   key permission, secret permissions, certificate permissions

 add access policy
 select principal
    user

------
Azure VM Disk Encryption
key vault
Access polices
  check - Azure Disk Encryption for volume encryption

azure cloud shell - powershell

az vm encryption enable -g testgrp --name newvm --disk-encryption-keyvault demovault9090

-------
key vault - Invoking via API calls

azure ad registration
  register an application_ postman
  http://localhost

api permissions
  add azure key vault
     user_impersonation

Authentication
  Access tokens

Certificates & secrets
  add secret

Postman
  Post  https://login.microsoftonline.com/<tenent ID>/oauth2/token
  Body
    key                 value
    grant_type          client_credentials
    client_id           <clientid>
    client_secret       <value>

   resource             https://vault.azure.net

send

copy token
---
key vault
  secret
    copy - <Secret Identifier>  https://demovault8000.vault.azure.ner/secret...
      
add access policy
  secret permission
     get, list
  select principal
     postman


   GET  <Secret Identifier>
   Headers
      key                 Value
      Authorization       Bearer <paste token>

send
---
Manged Service Identity
 key vault
 azure web app
=====================================
=====================================
Monitor
  Metrics
    Select Subscriptions
        Resource Type - Virtual Machine
Activity log
Alerts
  Create rule
    Resource - vm
    Condition - Signal type: Activity log
                  all administrative operations
    Action Groups - resource group
                   email

stop vm
you recieve a email
---
vm - Properties
      Resource ID: /subscriptions/20c6eec9-2d80-.....

azure cli
az monitor metrics alert create -n demoalert -g demogrp1 --scopes /subscriptions/20c6eec9-2d80-4700-b0f6-4fde579a8783/resourceGroups/demogrp1/providers/Microsoft.Compute/virtualMachines/linuxvm 
--condition "avg Percentage CPU > 80" --window-size 5m --evaluation-frequency 1m --description "High CPU"

--------------
Auto Scaling - Azure WebApp
//App service plan

scale out
custom scale
  add rule


scale out
manual scale
 instance count

scale out
custom scale
  add rule
     metric storage queue

---
Application Insights

sql database
 query editor
  CREATE TABLE Products (
    Id int NOT NULL IDENTITY,
    Name varchar(255) NOT NULL,
    price real,
    PRIMARY KEY (Id)
  );



insert into Products(Name,price) values('ProductA',40.99);

insert into Products(Name,price) values('ProductB',50.99);

insert into Products(Name,price) values('ProductC',60.99);

drop table Products;
vs run local


create web app
enable application insights

---
vs
Project
 Add Application insights Telemetry..
   Subscription
   Resource: existing resource- appinsight

run vs

--
View - Other Windows
           Application Insights Search
              Search debug session telematry

publish solution

---------
application insghts
live metrics
performance

  end-to-end transaction

-------
funnels
users flows
retention
-----
Availability
  Create test
    ping request
    url - webapp

------
traking user id
Application Insights


Overview

This is an application Performance Management service for web developers.
You can use this tool to monitor your applications.
It can help developers detect anomalies in the application.
It can help diagnose issues.
It can also help understand how users use your application.
It also helps you improve performance and usability of your application.

  *Different aspects that get monitored with Application Insights

Request rates, the response times and failure rates – This is done at the page level.
Exception recorded by your application.
Page views and their load performance as reported from the user’s browser.
User and session counts.
Performance counters of the underlying Windows or Linux Machines.
Diagnostic trace logs from your application.
Any custom events or metrics that the developer writes themselves in the code

   *Different ways to see how your site is behaving

 -Funnels

You can create a funnel from one stage to another stage of your application.
You can then see how users are progressing through the stages of the funnel.

 -User Flows

This helps visualize how users navigate between pages in your site. This can help answer question such as
Does the user navigate away from a page on your site
What do users click on a page on your site
Where are the places where users churn most on your site
Are there places where users repeat the same action over and over

 -Impact

This helps decide if a page is having an impact on your application.
It can help answer the question as to whether the page load time is impacting how many people convert on a page in the application.

 -Retention

This helps you understand how many users return to your application.
It can also help understand if users are able to perform certain tasks in your application.

 -Usage and estimation costs
If you go to your application insights resource and go to Usage and estimated costs, 
you will see your usage and estimated costs for using application insights
If you go onto Data retention, you can decide for how long you want to retain the data collected by Application Insights. The maximum retention period is 730 days
------------------------------------------
------------------------------------------
Azure Chache for Redis
create
console
---
vs
nuget package
Microsoft.Extensions.Caching.Redis
Microsoft.Extensions.Caching.Core

run

azure portal  - redis
console

>SCAN 0 COUNT 10

1) "0"
2) 1) "Count"
   2)  "masterProducts"

>HGETALL masterProducts
1)"data"
2) "[{}]"
3) "sldexp"
4) "-1"
5) "absexp"
6) "-1"

---
DEL masterProducts
SCAN 0 COUNT 10



------
StackExhange.Redis

---
vs publish web app
Azure Content Delivery Network

CDN
CDN Profile

endpoint - test url

application insights - availability
add test  (create 2 test with diferent locations)
  type: ping url test
  frecuency: 5minutes
  location: 
  
   type: ping url test
  frecuency: 5minutes
  location: 


----
endpoint
  caching rules

-------
Bpass caching for query strings
Cache every unique URL
ignore query strings

Caching rules
 Query string caching behavior

-----
Azure Front Door Service
This is a routing service that helps accelerate your application performance by
routing traffic bassed on performance of your endpoints.

backend pool
weightage
priority


---
create web app
vs publish web app

edit 

vs publish other web app

create front door
 add a frontend host:company

 add a backend pool   //add 2 backend
    app service
     backend host name          //endpoint
      priority
       weight


 add Routing rules
    HTTP only


test url

stop app service

view redirect to second web app


----------
Azure Front Door Caching and Compression
By default, Azure Front Door has the capability to deliver large files. If a file is large, it splits the file into chunks of 8 MB. 
If the file is not in the cache, the large file is retrieved from the origin. Once the first 8 MB chunk is received , 
it is sent to the user and then in parallel , the next 8 MB chunk is retrieved from the origin.

Azure Front Door also supports compression. It supports all of the following MIME types. But the file needs to be in size between 1 KB and 8 MB. 
It supports both gzip and Brotli compression.

"application/eot"
"application/font"
"application/font-sfnt"
"application/javascript"
"application/json"
"application/opentype"
"application/otf"
"application/pkcs7-mime"
"application/truetype"
"application/ttf",
"application/vnd.ms-fontobject"
"application/xhtml+xml"
"application/xml"
"application/xml+rss"
"application/x-font-opentype"
"application/x-font-truetype"
"application/x-font-ttf"
"application/x-httpd-cgi"
"application/x-mpegurl"
"application/x-opentype"
"application/x-otf"
"application/x-perl"
"application/x-ttf"
"application/x-javascript"
"font/eot"
"font/ttf"
"font/otf"
"font/opentype"
"image/svg+xml"
"text/css"
"text/csv"
"text/html"
"text/javascript"
"text/js", "text/plain"
"text/richtext"
"text/tab-separated-values"
"text/xml"
"text/x-script"
"text/x-component"
"text/x-java-source"

Transient faults

==========================
==========================
Azure Service Bus
fifo
topics - subscribers

create service bus

Subscription
  resource group

namespace name
location
pricing tier - standard

-
service bus namespace
Queues
  create queue
    name
    maxqueue size
      30 sec               //lock duration

-
Service Bus Explorer
  Content Type: Text/Plain
    send


send - receive - peek
-----
vs .net
nuget package
  Microsoft.Azure.ServiceBus


Service Bus Queue
Shared access polices
  add SAS Policy
     policy name:
        check send
        check listen

 select policy
   copy - primary connection string

run vs

-----------
Service Bus Queue
Propeties

vs .net
-------
Recieve and Delete Mode
appqueue


-----
Topic
add subscription
  name
   max delivery count: 3

--
vs .net
nuget package
  Microsoft.Azure.ServiceBus v4.1.3

service bus - topic-send
service bus - topic - receive  

---
Topic - Filters
Service Bus Subscription
 Overview
    SubscriptionA
       Add filter
         Name: messagefilter
         Filter Type: SQL Filter
         sys.MessageId = '1'

vs run
------
nuget package
  Microsoft.Azure.Management.

Shared access polices
  add SAS Policy
     policy name:
        check manage

   select policy
   copy - primary connection string

vs
CreateSubscription().Wait();
AddFilter().Wait();

----
Azure Service Bus - dead letter queue

30 seconds
check - Move expired messages to the dead-letter subqueue


vs .net - read dead letter message queue

----
Azure Service Bus - Duplicate Detection

1 minute  
check - enable duplicate detection
CorrelationId

--------------------
Azure Service Bus - Best Practices


When you have clients that send or receive messages from the Azure Service Bus, the clients normally of the type IQueueClient.
In the background these objects make use of the MessagingFactory object. This provides the internal management of connections.
Don’t close the connections directly after sending or receiving messages, since establishing a connection is an expensive operation.
Hence use the same client object for multiple operation
 
*Client batching 
can be done with the prior version of the SDK – WindowsAzure.ServiceBus.SDK. Client batching delays the sending of messages and instead sends pending messages as a batch.

*Batched store access 
is when the queue itself batches multiple messages before it is written to the internal store. This helps in better throughput.

*Enable Prefetch – 
Here the receiver quietly acquires more messages from the queue or topic subscription. This is up to a value defined by the PrefetchCount limit.
When a receiver wants to receive messages, the messages are received from the buffer based on the number of prefetched messages. Then additional messages are prefetched again in the background.
The issues with Prefetch. Here in ReceiveAndDelete mode , remember when the messages are prefetched, they are removed from the queue. If the application crashes before the messages are processed, the messages are lost.
You can set the Prefetch count for the clients. QueueClient.PrefetchCount or SubscriptionClient.PrefetchCount


*If you need to implement a high throughput queue with a small number of senders and receivers

Use multiple message factories to create senders
Use asynchronous operations

*If you want to decrease the latency of sending or receiving messages.

You can disable client batching of messages and batched store access.
If you have a single client, consider using a prefetch count up to 20 times the processing rate of the receiver.

*If you need to implement a high throughput queue with a large number of senders and small number of receivers

For the sender that resides in a different process, use only one single factory per process.
Use asynchronous operations and take advantage of client-side batching.
Leave the setting of batched store access enabled.
Set the prefetch count to 20 times that of the maximum processing rates of all receivers of a factory.

*If you need to implement a high throughput queue with a small number of senders and large number of receivers

If each receiver is in a different process , use only a single factory per process.
Leave batched store access enabled.
Set the prefetch count to a lower value. This is because you have multiple receivers.


*If you need to implement a high throughput topic with a small number of senders and small number of subscriptions

Increase the overall send rate by using multiple message factories to create senders
Increase the overall receive rate by using multiple message factories to create receivers
Use asynchronous operations and client-side batching.
Leave batched store access enabled.
Set the prefetch count to 20 times the maximum processing rates of all receivers of a factory

---
create queue -vs
---
Azure Service Bus - Azure CLI
You can use the following commands to work with the Azure Service Bus service

// You can set the following variables for your script

$namespace="appnamespace4000"
$resourcegrp="demogrp1"
$location="Central US"
$queuename="appqueue"
$topicname="apptopic"
$subscriptionName="SubscriptionA"

// You can use the following command to create a service bus namespace
az servicebus namespace create --name $namespace --resource-group $resourcegrp --location $location --sku Standard

// You can use the following command to create a service bus queue
az servicebus queue create --resource-group $resourcegrp --namespace-name $namespace --name $queuename --max-size 1024

// You can use the following command to create a service bus topic
az servicebus topic create --resource-group $resourcegrp --namespace-name $namespace --name $topicname --max-size 1024

// You can use the following command to create a service bus subscription
az servicebus topic subscription create --resource-group $resourcegrp --namespace-name $namespace --topic-name $topicname --name $subscriptionName

============
Azure Event Grid - Azure Function
---
Azure Event Grid Schema
--
service bus handler
az eventgrid event-subscription create --name queue-subscription --source-resource-id /subscriptions/20c6eec9-2d80-4700-b0f6-4fde579a8783/resourceGroups/demogrp1/providers/Microsoft.Storage/storageAccounts/demostore4000 --endpoint-type servicebusqueue --endpoint /subscriptions/20c6eec9-2d80-4700-b0f6-4fde579a8783/resourceGroups/demogrp1/providers/Microsoft.ServiceBus/namespaces/appnamespace4000/queues/appqueue

..
webhook 

----
- function as webhook
---
custom topics

---------
---------
Azure Events Hubs
create  event hubs

..net programs
send messages event hub
recieve messages event hub

nuget package
 Azure.Messaging.EventHubs

Shared access polices
----
partitions
  reader, 

--
readinf from an Offset

-------------------
--------------------
Azure API Managament
create SQL Database
CREATE TABLE Courses (
    Id int NOT NULL,
    Name varchar(255) NOT NULL,
    rating int,
    PRIMARY KEY (Id)
);

insert into Courses(Id,Name,rating) values(1,'AZ-900',4);

insert into Courses(Id,Name,rating) values(2,'AZ-204',5);

insert into Courses(Id,Name,rating) values(3,'AZ-500',6);

drop table Courses;



vs publish web app

postman
POST https://demoapi7000.azurewebsites.net/api/Courses
Body   
   raw   JSON
  {"id":4,"Name":"az-204","rating":5}


------
create api management

Create a blank API
  Displayname
  web service URL
  Base URL

Add operation
  Name: Get all courses
  URL GET /Courses

  test

GET
send
--
Postman
GET https://demoapi200.azure-api-net/Courses
key				Value
Ocp-Apim-Subscription-Key	80e23.....

Add operation
  Name: Get-a-course
  URL GET /Courses/{id}

Add operation
  Name: add-a-course
  URL POST  /api/Courses

---
API Mangement policies

select operation
  Inbound processing
  Outbound processing
    polices code editor
 

---
Rewrite URL's
<policies>
    <inbound>
        <base />
        <rewrite-uri template="@{
        return "/api/Courses"; }" />
    </inbound>
    <backend>
        <base />
    </backend>
    <outbound>
        <base />
    </outbound>
    <on-error>
        <base />
    </on-error>
</policies>
----
    <inbound>
        <base />
        <set-variable name="CourseId" value="1" />
        <rewrite-uri template="@{
        return "/api/Courses/"+ context.Variables.GetValueOrDefault<string>("CourseId"); }" />
    </inbound>
----
Conditions

    <inbound>
        <base />
        <set-variable name="Total" value="@(context.Request.Headers["total"][0])" />
        <choose>
            <when condition="@(int.Parse(context.Variables.GetValueorDefault<string>("Total"))<10">
		<rewrite-uri template="@{return "/api/Courses";}" />
	    </when>
	    <otherwise>
                <rewrite-uri template="@{return "/Home/Details/1";}"  />
		<set-backend-service base-url="https://demostaging4000.azurewebsites.net"/>
            </otherwise>
	</choose>
    </inbound>
-----
Outbound Rule
<policies>
    <inbound>
        <base />
        <rewrite-uri template="@{
        return "/api/Courses"; }" />
    </inbound>
    <backend>
        <base />
    </backend>
    <outbound>
        <base />
 <choose>
      <when condition="@(context.Response.StatusCode == 200)">
        <set-body>
          @{
          return "Error";
           }               
    </set-body>
      </when>
    </choose>
    </outbound>
    <on-error>
        <base />
    </on-error>
</policies>

----
OpenAPI Specification
nuget package
 Swashbuckle.AspNetCore

----
powershell

PowerShell commands that can be used to work with the API Management instance

// 1. Here we are setting PowerShell variables.
// This is the name we want to give to the API 
$appservicename = "core-management"

// Here we mention an existing resource group
$resourceGroupName = "demogrp1"

// Here we specify the location in which we want to create our API Management Instance
$location = "East US"

// Here we specify the organization name
$organisation = "CompanyA"

// Here we specify the organization email
$adminEmail = "admin@company.com"

// Here we are assuming that our API is using the Open API Specification
$Url="https://corewebapi4000.azurewebsites.net/swagger/v1/swagger.json"

// Here we are giving the location of an Azure Web App that is hosting our API
$webapi="https://corewebapi4000.azurewebsites.net"

// The below PowerShell command can be used to create a new API Management Instance
New-AzApiManagement -ResourceGroupName $resourceGroupName -Name $appservicename -Location $location -Organization $organisation -AdminEmail $adminEmail

// After we create the Azure API Management Instance, we then need to set the context
$context = New-AzApiManagementContext -ResourceGroupName $resourceGroupName -ServiceName $appservicename

// We can then import an API using our web api
$api = Import-AzApiManagementApi -Context $context -SpecificationUrl $Url -SpecificationFormat "OpenApi" -ServiceUrl $webapi

------------------
Azure Logic Apps

create container in storage account

create logic app

Blank logic app
  azure event grid 
     event type - Actionsuccess, Writesuccess, DeleteSuccess
  azure blob storage
     Blob name  - ID
     Blob content - EventType, Subject, Topic


 -------
with event grid
--------
azure functions

--------
azure blob storage

Connectors for Azure Logic Apps
https://docs.microsoft.com/en-us/azure/connectors/apis-list
         




====================
====================
Azure Web app log
 https://docs.microsoft.com/en-us/cli/azure/webapp/log?view=azure-cli-latest

az webapp log config

  [--docker-container-logging {filesystem, off}]

